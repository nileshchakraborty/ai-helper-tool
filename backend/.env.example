# Server Configuration
PORT=3000
NODE_ENV=development

# Database & Cache
REDIS_URL=redis://localhost:6379
DATABASE_URL=postgresql://dev:dev@localhost:5432/maccopilot

# JWT Secret (change in production!)
JWT_SECRET=supersecret_dev_key_change_in_prod

# AI Providers - set the ones you want to use
# Leave blank or remove if not using a specific provider

# OpenAI (https://platform.openai.com/api-keys)
OPENAI_API_KEY=

# Anthropic Claude (https://console.anthropic.com/)
ANTHROPIC_API_KEY=

# Ollama - Local AI (runs in Docker or locally)
# For Docker: http://ollama:11434
# For local: http://127.0.0.1:11434
OLLAMA_HOST=http://127.0.0.1:11434

# Ollama Model - which model to use
# Examples: llama3.2, deepseek-coder, codellama, llava (vision), mistral
OLLAMA_MODEL=llama3.2

# Ollama API Key (optional) - for remote Ollama servers requiring auth
# Leave blank for local Docker/localhost Ollama
OLLAMA_API_KEY=

# Google Gemini (Nano Banana) - for image generation (BACKUP)
# Get your key at: https://aistudio.google.com/apikey
GOOGLE_API_KEY=

# ComfyUI - Local Stable Diffusion (DEFAULT, free, unlimited)
# Install ComfyUI: https://github.com/comfyanonymous/ComfyUI
# Run with: python main.py --listen
COMFYUI_HOST=http://127.0.0.1:8188

# ========================================
# AI Framework Feature Flags
# ========================================

# Use LangGraph multi-agent orchestrator (instead of legacy)
USE_LANGGRAPH=true

# Use ChromaDB for RAG (retrieval-augmented generation)
USE_CHROMADB=true

# Use Neo4j for user progress tracking
USE_NEO4J=true

# ========================================
# Data Services (Docker)
# ========================================

# ChromaDB Vector Store
CHROMA_URL=http://localhost:8000

# Neo4j Knowledge Graph
NEO4J_URI=bolt://localhost:7687
NEO4J_USER=neo4j
NEO4J_PASSWORD=interview123
